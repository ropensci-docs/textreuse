[{"path":"https://docs.ropensci.org/textreuse/CONDUCT.html","id":null,"dir":"","previous_headings":"","what":"Contributor Code of Conduct","title":"Contributor Code of Conduct","text":"contributors maintainers project, pledge respect people contribute reporting issues, posting feature requests, updating documentation, submitting pull requests patches, activities. committed making participation project harassment-free experience everyone, regardless level experience, gender, gender identity expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion. Examples unacceptable behavior participants include use sexual language imagery, derogatory comments personal attacks, trolling, public private harassment, insults, unprofessional conduct. Project maintainers right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct. Project maintainers follow Code Conduct may removed project team. Instances abusive, harassing, otherwise unacceptable behavior may reported opening issue contacting one project maintainers. Code Conduct adapted Contributor Covenant (http:contributor-covenant.org), version 1.0.0, available http://contributor-covenant.org/version/1/0/0/","code":""},{"path":[]},{"path":"https://docs.ropensci.org/textreuse/articles/textreuse-introduction.html","id":"textreusetextdocument","dir":"Articles","previous_headings":"TextReuse classes","what":"TextReuseTextDocument","title":"Introduction to the textreuse package","text":"basic class provided package TextReuseTextDocument class. class contains text document metadata. document loaded, text also tokenized. (See section tokenizers .) tokens hashed using hash function. default hashes retained tokens discarded, since using hashes results significant memory savings. load file TextReuseTextDocument tokenize shingled n-grams, adding option retain tokens. can see details document accessor functions. derived S3 virtual class TextDocument NLP package. Notice ID assigned document based filename (without extension). name tokenizer hash functions also saved metadata. tokens() hashes() function return tokens hashes associated document. meta() function returns named list metadata fields. function called specific ID, meta(doc, \"myfield\"), value field returned. can also assign metadata whole specific field, example . addition content() function provides unprocessed text document. assumption want tokenize hash tokens start. , however, wish steps , can load document tokenizer = NULL, use tokenize() rehash() recompute tokens hashes. Note TextReuseTextDocument can actually contain two kinds hashes. hashes() accessor gives integer representations tokens document: 100,000 tokens document, 100,000 hashes. minhashes() accessor gives signature represents document whole specific tokens within . See minhash vignette details: vignette(\"textreuse-minhash\").","code":"library(textreuse) file <- system.file(\"extdata/ats/remember00palm.txt\",                      package = \"textreuse\") doc <- TextReuseTextDocument(file = file, meta = list(\"publisher\" = \"ATS\"),                              tokenizer = tokenize_ngrams, n = 5,                              keep_tokens = TRUE) doc ## TextReuseTextDocument ## file : /usr/local/lib/R/site-library/textreuse/extdata/ats/remember00palm.txt  ## hash_func : hash_string  ## id : remember00palm  ## publisher : ATS  ## tokenizer : tokenize_ngrams  ## content : Remember  ## By  ## Rat Palmer.  ## Boston:  ##  ## THE AMERICAN TRACT SOCI]  ##  ## Depositories, 28 Cornhill, Boston ; and 13 Biblb House,  ## Astor Place, New York.  ## Entered, according to Act of Congress, in the year 1865 meta(doc) ## $file ## [1] \"/usr/local/lib/R/site-library/textreuse/extdata/ats/remember00palm.txt\" ##  ## $hash_func ## [1] \"hash_string\" ##  ## $id ## [1] \"remember00palm\" ##  ## $publisher ## [1] \"ATS\" ##  ## $tokenizer ## [1] \"tokenize_ngrams\" meta(doc, \"id\") ## [1] \"remember00palm\" meta(doc, \"date\") <- 1865 head(tokens(doc)) ## [1] \"remember by rat palmer boston\"        ## [2] \"by rat palmer boston the\"             ## [3] \"rat palmer boston the american\"       ## [4] \"palmer boston the american tract\"     ## [5] \"boston the american tract soci\"       ## [6] \"the american tract soci depositories\" head(hashes(doc)) ## [1]  1603439917 -1208020871  1117958556 -1973578767  1310564160  1350652178 wordcount(doc) ## [1] 11399"},{"path":"https://docs.ropensci.org/textreuse/articles/textreuse-introduction.html","id":"textreusecorpus","dir":"Articles","previous_headings":"TextReuse classes","what":"TextReuseCorpus","title":"Introduction to the textreuse package","text":"class TextReuseCorpus provides list TextReuseTextDocuments. derives S3 virtual class Corpus tm package. can created directory files (providing vector paths files). names items TextReuseCorpus IDs documents. can use IDs subset corpus retrieve specific documents. Accessor functions meta(), tokens(), hashes(), wordcount() methods work corpora. Note creating corpus, short empty documents skipped warning. document must enough words create least two n-grams. example, five-grams desired, document must least six words.","code":"dir <- system.file(\"extdata/ats\", package = \"textreuse\") corpus <- TextReuseCorpus(dir = dir, tokenizer = tokenize_ngrams, n = 5,                           progress = FALSE) corpus ## TextReuseCorpus ## Number of documents: 8  ## hash_func : hash_string  ## tokenizer : tokenize_ngrams names(corpus) ## [1] \"calltounconv00baxt\"        \"gospeltruth00whit\"         ## [3] \"lifeofrevrichard00baxt\"    \"memoirjamesbrai00ricegoog\" ## [5] \"practicalthought00nev\"     \"remember00palm\"            ## [7] \"remembermeorholy00palm\"    \"thoughtsonpopery00nevi\" corpus[[\"remember00palm\"]] ## TextReuseTextDocument ## file : /usr/local/lib/R/site-library/textreuse/extdata/ats/remember00palm.txt  ## hash_func : hash_string  ## id : remember00palm  ## minhash_func :  ## tokenizer : tokenize_ngrams  ## content : Remember  ## By  ## Rat Palmer.  ## Boston:  ##  ## THE AMERICAN TRACT SOCI]  ##  ## Depositories, 28 Cornhill, Boston ; and 13 Biblb House,  ## Astor Place, New York.  ## Entered, according to Act of Congress, in the year 1865 corpus[c(\"calltounconv00baxt\", \"lifeofrevrichard00baxt\")] ## TextReuseCorpus ## Number of documents: 2  ## hash_func : hash_string  ## tokenizer : tokenize_ngrams wordcount(corpus) ##        calltounconv00baxt         gospeltruth00whit    lifeofrevrichard00baxt  ##                    134616                     16593                     44283  ## memoirjamesbrai00ricegoog     practicalthought00nev            remember00palm  ##                    131939                    124544                     11399  ##    remembermeorholy00palm    thoughtsonpopery00nevi  ##                     11532                     64758"},{"path":"https://docs.ropensci.org/textreuse/articles/textreuse-introduction.html","id":"tokenizers","dir":"Articles","previous_headings":"","what":"Tokenizers","title":"Introduction to the textreuse package","text":"One steps performed loading TextReuseTextDocument, either individual corpus, tokenization. Tokenization breaks text pieces, often overlapping. pieces features compared measuring document similarity. textreuse package provides number tokenizers. can write tokenizers use tokenizers packages. accept character vector first argument. example, write tokenizer function using  package splits text new lines, perhaps useful poetry. Notice function takes single string returns character vector one element line. (robust tokenizer might strip blank lines punctuation, include option lowercasing text, check validity arguments.)","code":"text <- \"How many roads must a man walk down\\nBefore you'll call him a man?\"  tokenize_words(text) ##  [1] \"how\"    \"many\"   \"roads\"  \"must\"   \"a\"      \"man\"    \"walk\"   \"down\"   ##  [9] \"before\" \"you'll\" \"call\"   \"him\"    \"a\"      \"man\" tokenize_sentences(text) ## [1] \"how many roads must a man walk down\" \"before you ll call him a man\" tokenize_ngrams(text, n = 3) ##  [1] \"how many roads\"     \"many roads must\"    \"roads must a\"       ##  [4] \"must a man\"         \"a man walk\"         \"man walk down\"      ##  [7] \"walk down before\"   \"down before you'll\" \"before you'll call\" ## [10] \"you'll call him\"    \"call him a\"         \"him a man\" tokenize_skip_ngrams(text, n = 3, k = 2) ##  [1] \"how must walk\"      \"many a down\"        \"roads man before\"   ##  [4] \"must walk you'll\"   \"a down call\"        \"man before him\"     ##  [7] \"walk you'll a\"      \"down call man\"      \"how roads a\"        ## [10] \"many must man\"      \"roads a walk\"       \"must man down\"      ## [13] \"a walk before\"      \"man down you'll\"    \"walk before call\"   ## [16] \"down you'll him\"    \"before call a\"      \"you'll him man\"     ## [19] \"how many roads\"     \"many roads must\"    \"roads must a\"       ## [22] \"must a man\"         \"a man walk\"         \"man walk down\"      ## [25] \"walk down before\"   \"down before you'll\" \"before you'll call\" ## [28] \"you'll call him\"    \"call him a\"         \"him a man\" poem <- \"Roses are red\\nViolets are blue\\nI like using R\\nAnd you should too\" cat(poem) ## Roses are red ## Violets are blue ## I like using R ## And you should too tokenize_lines <- function(string) {   stringr::str_split(string, \"\\n+\")[[1]] }  tokenize_lines(poem) ## [1] \"Roses are red\"      \"Violets are blue\"   \"I like using R\"     ## [4] \"And you should too\""},{"path":"https://docs.ropensci.org/textreuse/articles/textreuse-introduction.html","id":"hash-functions","dir":"Articles","previous_headings":"","what":"Hash functions","title":"Introduction to the textreuse package","text":"package provides one function hash tokens integers, hash_string(). can write hash functions, use provided digest package.","code":"hash_string(tokenize_words(text)) ##  [1] -1918507530  -727342581 -1125663377   258658140   -11612900  1695112593 ##  [7] -1411833952 -1120374007   888404863  -693828095  -290800936  2118188342 ## [13]   -11612900  1695112593"},{"path":"https://docs.ropensci.org/textreuse/articles/textreuse-introduction.html","id":"comparison-functions","dir":"Articles","previous_headings":"","what":"Comparison functions","title":"Introduction to the textreuse package","text":"package provides number comparison functions measuring similarity. functions take either set (token counted one time) bag (token counted many times appears) compares another set bag. See documentation ?similarity-functions details measured functions. can write similarity functions, accept two sets bags, b, work character numeric vectors, since used either tokens hashes tokens, return single numeric score comparison. need implement method TextReuseTextDocument class.","code":"a <- tokenize_words(paste(\"How does it feel, how does it feel?\",                           \"To be without a home\",                           \"Like a complete unknown, like a rolling stone\")) b <- tokenize_words(paste(\"How does it feel, how does it feel?\",                           \"To be on your own, with no direction home\",                           \"A complete unknown, like a rolling stone\"))  jaccard_similarity(a, b) ## [1] 0.65 jaccard_dissimilarity(a, b) ## [1] 0.35 jaccard_bag_similarity(a, b) ## [1] 0.4 ratio_of_matches(a, b) ## [1] 0.75"},{"path":"https://docs.ropensci.org/textreuse/articles/textreuse-introduction.html","id":"parallelization","dir":"Articles","previous_headings":"","what":"Parallelization","title":"Introduction to the textreuse package","text":"package use multiple cores functions option set. benefits corpus loading tokenizing functions, often slowest parts analysis. implemented parallel package, work Windows machines. (Regardless options set, package never attempt parallelize computations Windows.) use parallel option, must specify number CPU cores wish use: option set, package use multiple cores possible. can figure many cores computer parallel::detectCores(). See help(package = \"parallel\") details.","code":"options(\"mc.cores\" = 4L)"},{"path":"https://docs.ropensci.org/textreuse/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Lincoln Mullen. Author, maintainer.","code":""},{"path":"https://docs.ropensci.org/textreuse/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Mullen L (2024). textreuse: Detect Text Reuse Document Similarity. R package version 0.1.5, https://github.com/ropensci/textreuse, https://docs.ropensci.org/textreuse.","code":"@Manual{,   title = {textreuse: Detect Text Reuse and Document Similarity},   author = {Lincoln Mullen},   year = {2024},   note = {R package version 0.1.5, https://github.com/ropensci/textreuse},   url = {https://docs.ropensci.org/textreuse}, }"},{"path":[]},{"path":"https://docs.ropensci.org/textreuse/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Detect Text Reuse and Document Similarity","text":"R package provides set functions measuring similarity among documents detecting passages reused. implements shingled n-gram, skip n-gram, tokenizers; similarity/dissimilarity functions; pairwise comparisons; minhash locality sensitive hashing algorithms; version Smith-Waterman local alignment algorithm suitable natural language. broadly useful , example, detecting duplicate documents corpus prior text analysis, identifying borrowed passages texts. classes provides package follow model natural language processing packages R, especially NLP tm packages. (However, package dependency Java, make easier install.)","code":""},{"path":"https://docs.ropensci.org/textreuse/index.html","id":"citation","dir":"","previous_headings":"Overview","what":"Citation","title":"Detect Text Reuse and Document Similarity","text":"use package scholarly research, appreciate citation.","code":"citation(\"textreuse\") #>  #> To cite package 'textreuse' in publications use: #>  #>   Lincoln Mullen (2020). textreuse: Detect Text Reuse and Document #>   Similarity. https://docs.ropensci.org/textreuse, #>   https://github.com/ropensci/textreuse. #>  #> A BibTeX entry for LaTeX users is #>  #>   @Manual{, #>     title = {textreuse: Detect Text Reuse and Document Similarity}, #>     author = {Lincoln Mullen}, #>     year = {2020}, #>     note = {https://docs.ropensci.org/textreuse, https://github.com/ropensci/textreuse}, #>   }"},{"path":"https://docs.ropensci.org/textreuse/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Detect Text Reuse and Document Similarity","text":"install package CRAN: install development version GitHub, use devtools.","code":"install.packages(\"textreuse\") # install.packages(\"devtools\") devtools::install_github(\"ropensci/textreuse\", build_vignettes = TRUE)"},{"path":"https://docs.ropensci.org/textreuse/index.html","id":"examples","dir":"","previous_headings":"","what":"Examples","title":"Detect Text Reuse and Document Similarity","text":"three main approaches one may take using package: pairwise comparisons, minhashing/locality sensitive hashing, extracting matching passages text alignment. See introductory vignette description classes provided package.","code":"vignette(\"textreuse-introduction\", package = \"textreuse\")"},{"path":"https://docs.ropensci.org/textreuse/index.html","id":"pairwise-comparisons","dir":"","previous_headings":"Examples","what":"Pairwise comparisons","title":"Detect Text Reuse and Document Similarity","text":"example load tiny corpus three documents. documents drawn Kellen Funk’s research propagation legal codes civil procedure nineteenth-century United States. loaded three documents corpus, involves tokenizing text hashing tokens. can inspect corpus whole individual documents make . Now can compare documents one another. pairwise_compare() function applies comparison function (case, jaccard_similarity()) every pair documents. result matrix scores. expect, documents similar others . can convert matrix data frame pairs scores prefer. See pairwise vignette fuller description.","code":"library(textreuse) dir <- system.file(\"extdata/legal\", package = \"textreuse\") corpus <- TextReuseCorpus(dir = dir, meta = list(title = \"Civil procedure\"),                           tokenizer = tokenize_ngrams, n = 7) corpus #> TextReuseCorpus #> Number of documents: 3  #> hash_func : hash_string  #> title : Civil procedure  #> tokenizer : tokenize_ngrams names(corpus) #> [1] \"ca1851-match\"   \"ca1851-nomatch\" \"ny1850-match\" corpus[[\"ca1851-match\"]] #> TextReuseTextDocument #> file : /Users/lmullen/R/library/textreuse/extdata/legal/ca1851-match.txt  #> hash_func : hash_string  #> id : ca1851-match  #> minhash_func :  #> tokenizer : tokenize_ngrams  #> content : § 4. Every action shall be prosecuted in the name of the real party #> in interest, except as otherwise provided in this Act. #>  #> § 5. In the case of an assignment of a thing in action, the action by #> the as comparisons <- pairwise_compare(corpus, jaccard_similarity) comparisons #>                ca1851-match ca1851-nomatch ny1850-match #> ca1851-match             NA              0    0.3842549 #> ca1851-nomatch           NA             NA    0.0000000 #> ny1850-match             NA             NA           NA pairwise_candidates(comparisons) #> # A tibble: 3 x 3 #>   a              b              score #> * <chr>          <chr>          <dbl> #> 1 ca1851-match   ca1851-nomatch 0     #> 2 ca1851-match   ny1850-match   0.384 #> 3 ca1851-nomatch ny1850-match   0 vignette(\"textreuse-pairwise\", package = \"textreuse\")"},{"path":"https://docs.ropensci.org/textreuse/index.html","id":"minhashing-and-locality-sensitive-hashing","dir":"","previous_headings":"Examples","what":"Minhashing and locality sensitive hashing","title":"Detect Text Reuse and Document Similarity","text":"Pairwise comparisons can time-consuming grow geometrically size corpus. (corpus 10 documents require least 45 comparisons; corpus 100 documents require 4,950 comparisons; corpus 1,000 documents require 499,500 comparisons.) ’s package implements minhash locality sensitive hashing algorithms, can detect candidate pairs much faster pairwise comparisons corpora significant size. example load small corpus ten documents published American Tract Society. also create minhash function, represents entire document (regardless length) fixed number integer hashes. create corpus, documents minhash signature. Now can calculate potential matches, extract candidates, apply comparison function just candidates. details, see minhash vignette.","code":"dir <- system.file(\"extdata/ats\", package = \"textreuse\") minhash <- minhash_generator(200, seed = 235) ats <- TextReuseCorpus(dir = dir,                        tokenizer = tokenize_ngrams, n = 5,                        minhash_func = minhash) buckets <- lsh(ats, bands = 50, progress = FALSE) candidates <- lsh_candidates(buckets) scores <- lsh_compare(candidates, ats, jaccard_similarity, progress = FALSE) scores #> # A tibble: 2 x 3 #>   a                     b                      score #>   <chr>                 <chr>                  <dbl> #> 1 practicalthought00nev thoughtsonpopery00nevi 0.463 #> 2 remember00palm        remembermeorholy00palm 0.701 vignette(\"textreuse-minhash\", package = \"textreuse\")"},{"path":"https://docs.ropensci.org/textreuse/index.html","id":"text-alignment","dir":"","previous_headings":"Examples","what":"Text alignment","title":"Detect Text Reuse and Document Similarity","text":"can also extract optimal alignment documents version Smith-Waterman algorithm, used protein sequence alignment, adapted natural language. longest matching substring according scoring values extracted, variations alignment marked. details, see text alignment vignette.","code":"a <- \"'How do I know', she asked, 'if this is a good match?'\" b <- \"'This is a match', he replied.\" align_local(a, b) #> TextReuse alignment #> Alignment score: 7  #> Document A: #> this is a good match #>  #> Document B: #> This is a #### match vignette(\"textreuse-alignment\", package = \"textreuse\")"},{"path":"https://docs.ropensci.org/textreuse/index.html","id":"parallel-processing","dir":"","previous_headings":"Examples","what":"Parallel processing","title":"Detect Text Reuse and Document Similarity","text":"Loading corpus creating tokens benefit using multiple cores, available. (works non-Windows machines.) use multiple cores, set options(\"mc.cores\" = 4L), number many cores wish use.","code":""},{"path":"https://docs.ropensci.org/textreuse/index.html","id":"contributing-and-acknowledgments","dir":"","previous_headings":"Examples","what":"Contributing and acknowledgments","title":"Detect Text Reuse and Document Similarity","text":"Please note project released Contributor Code Conduct. participating project agree abide terms. Thanks Noam Ross thorough peer review package rOpenSci.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseCorpus.html","id":null,"dir":"Reference","previous_headings":"","what":"TextReuseCorpus — TextReuseCorpus","title":"TextReuseCorpus — TextReuseCorpus","text":"constructor function TextReuseCorpus, modeled virtual S3 class Corpus tm package. object TextReuseCorpus, basically list containing objects class TextReuseTextDocument. Arguments passed along constructor function. create corpus, can pass either character vector paths text files using paths = parameter, directory containing text files (extension) using dir = parameter, character vector documents using text =  parameter, element characer vector document. character vector passed text =  names, names used document IDs. Otherwise, IDs assigned documents. one paths, dir, text parameters specified.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseCorpus.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"TextReuseCorpus — TextReuseCorpus","text":"","code":"TextReuseCorpus(   paths,   dir = NULL,   text = NULL,   meta = list(),   progress = interactive(),   tokenizer = tokenize_ngrams,   ...,   hash_func = hash_string,   minhash_func = NULL,   keep_tokens = FALSE,   keep_text = TRUE,   skip_short = TRUE )  is.TextReuseCorpus(x)  skipped(x)"},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseCorpus.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"TextReuseCorpus — TextReuseCorpus","text":"paths character vector paths files opened. dir path directory text files. text character vector (possibly named) documents. meta list named elements metadata associated corpus. progress Display progress bar loading files. tokenizer function split text tokens. See tokenizers. value NULL, tokenizing hashing skipped. ... Arguments passed tokenizer. hash_func function hash tokens. See hash_string. minhash_func function create minhash signatures document. See minhash_generator. keep_tokens tokens saved documents returned discarded? keep_text text saved documents returned discarded? skip_short short documents skipped? (See details.) x R object check.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseCorpus.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"TextReuseCorpus — TextReuseCorpus","text":"skip_short = TRUE, function skip short   empty documents. short document one two words   create least two n-grams. example, five-grams desired,   document must least six words long. value n   provided, function assumes value n = 3. warning   printed document ID skipped document. Use   skipped() get IDs skipped documents. function use multiple cores non-Windows machines   \"mc.cores\" option set. example, use four cores:   options(\"mc.cores\" = 4L).","code":""},{"path":[]},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseCorpus.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"TextReuseCorpus — TextReuseCorpus","text":"","code":"dir <- system.file(\"extdata/legal\", package = \"textreuse\") corpus <- TextReuseCorpus(dir = dir, meta = list(\"description\" = \"Field Codes\")) # Subset by position or file name corpus[[1]] #> TextReuseTextDocument #> file : /usr/local/lib/R/site-library/textreuse/extdata/legal/ca1851-match.txt  #> hash_func : hash_string  #> id : ca1851-match  #> minhash_func :  #> tokenizer : tokenize_ngrams  #> content : § 4. Every action shall be prosecuted in the name of the real party #> in interest, except as otherwise provided in this Act. #>  #> § 5. In the case of an assignment of a thing in action, the action by #> the as names(corpus) #> [1] \"ca1851-match\"   \"ca1851-nomatch\" \"ny1850-match\"   corpus[[\"ca1851-match\"]] #> TextReuseTextDocument #> file : /usr/local/lib/R/site-library/textreuse/extdata/legal/ca1851-match.txt  #> hash_func : hash_string  #> id : ca1851-match  #> minhash_func :  #> tokenizer : tokenize_ngrams  #> content : § 4. Every action shall be prosecuted in the name of the real party #> in interest, except as otherwise provided in this Act. #>  #> § 5. In the case of an assignment of a thing in action, the action by #> the as"},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseTextDocument-accessors.html","id":null,"dir":"Reference","previous_headings":"","what":"Accessors for TextReuse objects — TextReuseTextDocument-accessors","title":"Accessors for TextReuse objects — TextReuseTextDocument-accessors","text":"Accessor functions read write components TextReuseTextDocument TextReuseCorpus objects.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseTextDocument-accessors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Accessors for TextReuse objects — TextReuseTextDocument-accessors","text":"","code":"tokens(x)  tokens(x) <- value  hashes(x)  hashes(x) <- value  minhashes(x)  minhashes(x) <- value"},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseTextDocument-accessors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Accessors for TextReuse objects — TextReuseTextDocument-accessors","text":"x object access. value value assign.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseTextDocument-accessors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Accessors for TextReuse objects — TextReuseTextDocument-accessors","text":"Either vector named list vectors.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseTextDocument.html","id":null,"dir":"Reference","previous_headings":"","what":"TextReuseTextDocument — TextReuseTextDocument","title":"TextReuseTextDocument — TextReuseTextDocument","text":"constructor function TextReuseTextDocument objects. class used comparing documents.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseTextDocument.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"TextReuseTextDocument — TextReuseTextDocument","text":"","code":"TextReuseTextDocument(   text,   file = NULL,   meta = list(),   tokenizer = tokenize_ngrams,   ...,   hash_func = hash_string,   minhash_func = NULL,   keep_tokens = FALSE,   keep_text = TRUE,   skip_short = TRUE )  is.TextReuseTextDocument(x)  has_content(x)  has_tokens(x)  has_hashes(x)  has_minhashes(x)"},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseTextDocument.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"TextReuseTextDocument — TextReuseTextDocument","text":"text character vector containing text document. argument can skipped supplying file. file path text file, text provided. meta list named elements metadata associated document. document created using text parameter, must provide id field, e.g., meta = list(id = \"my_id\"). document created using file, ID created file name. tokenizer function split text tokens. See tokenizers. value NULL, tokenizing hashing skipped. ... Arguments passed tokenizer. hash_func function hash tokens. See hash_string. minhash_func function create minhash signatures document. See minhash_generator. keep_tokens tokens saved document returned discarded? keep_text text saved document returned discarded? skip_short short documents skipped? (See details.) x R object check.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseTextDocument.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"TextReuseTextDocument — TextReuseTextDocument","text":"object class TextReuseTextDocument. object inherits   virtual S3 class TextDocument NLP   package. contains following elements: content   text document. tokens tokens created text. hashes Hashes created tokens. minhashes minhash   signature document. metadata document metadata,   including filename () file.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseTextDocument.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"TextReuseTextDocument — TextReuseTextDocument","text":"constructor function follows three-step process. reads   text, either file memory. tokenizes text.   hashes tokens. comparison functions package   rely hashes make comparison. passing FALSE   keep_tokens keep_text, can avoid saving   objects, can result significant memory savings large corpora. skip_short = TRUE, function return NULL   short empty documents. short document one two   words create least two n-grams. example, five-grams   desired, document must least six words long. value   n provided, function assumes value n = 3.   warning printed document ID skipped document.","code":""},{"path":[]},{"path":"https://docs.ropensci.org/textreuse/reference/TextReuseTextDocument.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"TextReuseTextDocument — TextReuseTextDocument","text":"","code":"file <- system.file(\"extdata/legal/ny1850-match.txt\", package = \"textreuse\") doc  <- TextReuseTextDocument(file = file, meta = list(id = \"ny1850\")) print(doc) #> TextReuseTextDocument #> file : /usr/local/lib/R/site-library/textreuse/extdata/legal/ny1850-match.txt  #> hash_func : hash_string  #> id : ny1850  #> tokenizer : tokenize_ngrams  #> content : § 597. Every action must be prosecuted in the name #> of the real party in interest, except as otherwise provided in section 599. #>  #> ..a— #>  #> 5./imended Code, § 111. #>  #> §598. In the case of an assignment of a t meta(doc) #> $file #> [1] \"/usr/local/lib/R/site-library/textreuse/extdata/legal/ny1850-match.txt\" #>  #> $hash_func #> [1] \"hash_string\" #>  #> $id #> [1] \"ny1850\" #>  #> $tokenizer #> [1] \"tokenize_ngrams\" #>  head(tokens(doc)) #> NULL head(hashes(doc)) #> [1]  -221637926   996319810  -419169523   -37457565 -1872322441    75599962 if (FALSE) { content(doc) }"},{"path":"https://docs.ropensci.org/textreuse/reference/align_local.html","id":null,"dir":"Reference","previous_headings":"","what":"Local alignment of natural language texts — align_local","title":"Local alignment of natural language texts — align_local","text":"function takes two texts, either strings TextReuseTextDocument objects, finds optimal local alignment texts. local alignment finds best matching subset two documents. function adapts Smith-Waterman algorithm, used genetic sequencing, use natural language. compare texts word word (comparison case-insensitive) scores according set parameters. parameters define score match, penalties mismatch opening gap (.e., first mismatch potential sequence). function reports optimal local alignment. subset documents match included. Insertions deletions text reported edit_mark character.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/align_local.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Local alignment of natural language texts — align_local","text":"","code":"align_local(   a,   b,   match = 2L,   mismatch = -1L,   gap = -1L,   edit_mark = \"#\",   progress = interactive() )"},{"path":"https://docs.ropensci.org/textreuse/reference/align_local.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Local alignment of natural language texts — align_local","text":"character vector length one, TextReuseTextDocument. b character vector length one, TextReuseTextDocument. match score assign matching word. positive integer. mismatch score assign mismatching word. negative integer zero. gap penalty opening gap sequence. negative integer zero. edit_mark single character used displaying displaying insertions/deletions documents. progress Display progress bar messages computing alignment.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/align_local.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Local alignment of natural language texts — align_local","text":"list class textreuse_alignment. list contains   several elements: a_edit b_edit:   Character vectors sequences edits marked. score:   score optimal alignment.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/align_local.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Local alignment of natural language texts — align_local","text":"compute time function proportional product lengths two documents. Thus, longer documents take considerably time compute. function tested pairs documents containing 25 thousand words . function reports multiple optimal alignments, likely strong match document. score reported local alignment dependent size documents strength match, well parameters match, mismatch, gap penalties, scores directly comparable.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/align_local.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Local alignment of natural language texts — align_local","text":"useful description algorithm, see     post. application Smith-Waterman algorithm natural   language, see David . Smith, Ryan Cordell, Elizabeth Maddock Dillon,   \"Infectious Texts: Modeling Text Reuse Nineteenth-Century Newspapers.\"   IEEE International Conference Big Data, 2013,   http://hdl.handle.net/2047/d20004858.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/align_local.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Local alignment of natural language texts — align_local","text":"","code":"align_local(\"The answer is blowin' in the wind.\",             \"As the Bob Dylan song says, the answer is blowing in the wind.\") #> TextReuse alignment #> Alignment score: 11  #> Document A: #> The answer is blowin ####### in the wind #>  #> Document B: #> the answer is ###### blowing in the wind  # Example of matching documents from a corpus dir <- system.file(\"extdata/legal\", package = \"textreuse\") corpus <- TextReuseCorpus(dir = dir, progress = FALSE) alignment <- align_local(corpus[[\"ca1851-match\"]], corpus[[\"ny1850-match\"]]) str(alignment) #> List of 3 #>  $ a_edits: chr \"Every action shall #### be prosecuted in the name of the real party in interest except as otherwise provided in\"| __truncated__ #>  $ b_edits: chr \"Every action ##### must be prosecuted in the name of the real party in interest except as otherwise provided in\"| __truncated__ #>  $ score  : int 1032 #>  - attr(*, \"class\")= chr [1:2] \"textreuse_alignment\" \"list\""},{"path":"https://docs.ropensci.org/textreuse/reference/as.matrix.textreuse_candidates.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert candidates data frames to other formats — as.matrix.textreuse_candidates","title":"Convert candidates data frames to other formats — as.matrix.textreuse_candidates","text":"S3 methods convert textreuse_candidates object matrix.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/as.matrix.textreuse_candidates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert candidates data frames to other formats — as.matrix.textreuse_candidates","text":"","code":"# S3 method for textreuse_candidates as.matrix(x, ...)"},{"path":"https://docs.ropensci.org/textreuse/reference/as.matrix.textreuse_candidates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert candidates data frames to other formats — as.matrix.textreuse_candidates","text":"x object class textreuse_candidates. ... Additional arguments.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/as.matrix.textreuse_candidates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert candidates data frames to other formats — as.matrix.textreuse_candidates","text":"similarity matrix row column names containing document IDs.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/filenames.html","id":null,"dir":"Reference","previous_headings":"","what":"Filenames from paths — filenames","title":"Filenames from paths — filenames","text":"function takes character vector paths returns just file name, default without extension. TextReuseCorpus uses paths files corpus names list. function intended turn paths manageable identifiers.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/filenames.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filenames from paths — filenames","text":"","code":"filenames(paths, extension = FALSE)"},{"path":"https://docs.ropensci.org/textreuse/reference/filenames.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filenames from paths — filenames","text":"paths character vector paths. extension file extension preserved?","code":""},{"path":[]},{"path":"https://docs.ropensci.org/textreuse/reference/filenames.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filenames from paths — filenames","text":"","code":"paths <- c(\"corpus/one.txt\", \"corpus/two.md\", \"corpus/three.text\") filenames(paths) #> [1] \"one\"   \"two\"   \"three\" filenames(paths, extension = TRUE) #> [1] \"one.txt\"    \"two.md\"     \"three.text\""},{"path":"https://docs.ropensci.org/textreuse/reference/hash_string.html","id":null,"dir":"Reference","previous_headings":"","what":"Hash a string to an integer — hash_string","title":"Hash a string to an integer — hash_string","text":"Hash string integer","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/hash_string.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Hash a string to an integer — hash_string","text":"","code":"hash_string(x)"},{"path":"https://docs.ropensci.org/textreuse/reference/hash_string.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Hash a string to an integer — hash_string","text":"x character vector hashed.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/hash_string.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Hash a string to an integer — hash_string","text":"vector integer hashes.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/hash_string.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Hash a string to an integer — hash_string","text":"","code":"s <- c(\"How\", \"many\", \"roads\", \"must\", \"a\", \"man\", \"walk\", \"down\") hash_string(s) #> [1] -1265721719  -727342581 -1125663377   258658140   -11612900  1695112593 #> [7] -1411833952 -1120374007"},{"path":"https://docs.ropensci.org/textreuse/reference/lsh.html","id":null,"dir":"Reference","previous_headings":"","what":"Locality sensitive hashing for minhash — lsh","title":"Locality sensitive hashing for minhash — lsh","text":"Locality sensitive hashing (LSH) discovers potential matches among corpus documents quickly, likely pairs can compared.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Locality sensitive hashing for minhash — lsh","text":"","code":"lsh(x, bands, progress = interactive())"},{"path":"https://docs.ropensci.org/textreuse/reference/lsh.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Locality sensitive hashing for minhash — lsh","text":"x TextReuseCorpus TextReuseTextDocument. bands number bands use locality sensitive hashing. number hashes documents corpus must evenly divisible number bands. See lsh_threshold lsh_probability guidance selecting number bands hashes. progress Display progress bar comparing documents.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Locality sensitive hashing for minhash — lsh","text":"data frame (additional class lsh_buckets),  containing column document IDs column LSH  signatures, buckets.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Locality sensitive hashing for minhash — lsh","text":"Locality sensitive hashing technique detecting document  similarity require pairwise comparisons. comparing pairs  documents, number pairs grows rapidly, smallest  corpora can compared pairwise reasonable amount computation time.  Locality sensitive hashing, hand, takes document  tokenized hashed using minhash algorithm. (See  minhash_generator.) set minhash signatures  broken bands comprised certain number rows. (example, 200  minhash signatures might broken 20 bands containing 10  rows.) band hashed bucket. Documents identical rows  band hashed bucket. likelihood document  marked potential duplicate proportional number  bands inversely proportional number rows band. function returns data frame additional class  lsh_buckets. LSH technique requires signatures  document calculated . possible, long one uses  minhash function number bands, combine outputs  function different times. output can thus treated  kind cache LSH signatures. extract pairs documents output function, see  lsh_candidates.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Locality sensitive hashing for minhash — lsh","text":"Jure Leskovec, Anand Rajaraman, Jeff Ullman,  Mining Massive Datasets  (Cambridge University Press, 2011), ch. 3. See also Matthew Casperson,  \"Minhash   Dummies\" (November 14, 2013).","code":""},{"path":[]},{"path":"https://docs.ropensci.org/textreuse/reference/lsh.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Locality sensitive hashing for minhash — lsh","text":"","code":"dir <- system.file(\"extdata/legal\", package = \"textreuse\") minhash <- minhash_generator(200, seed = 235) corpus <- TextReuseCorpus(dir = dir,                           tokenizer = tokenize_ngrams, n = 5,                           minhash_func = minhash) buckets <- lsh(corpus, bands = 50) #> Warning: `gather_()` was deprecated in tidyr 1.2.0. #> ℹ Please use `gather()` instead. #> ℹ The deprecated feature was likely used in the textreuse package. #>   Please report the issue at <https://github.com/ropensci/textreuse/issues>. buckets #> # A tibble: 150 × 2 #>    doc          buckets                          #>    <chr>        <chr>                            #>  1 ca1851-match af692f79ec4b385468884a7310754366 #>  2 ca1851-match 7c25ad33c10068ed1d00e4a497be465e #>  3 ca1851-match b36543167c07507579ecabbe47675f5f #>  4 ca1851-match 68c1a9136b1aaf612b6982cf65db4dd9 #>  5 ca1851-match 99239407d33e84499e9b7fde16f43948 #>  6 ca1851-match 0ccad6c986d21807e9554b0757f6d156 #>  7 ca1851-match c32331c5c97d4d6b78fff25131aed561 #>  8 ca1851-match caac15d4cafc556ea08288ead841fd40 #>  9 ca1851-match d5868fcdf1f11f33859f640ac51e1931 #> 10 ca1851-match 999835f346cf6af6bf68bd96543997cd #> # ℹ 140 more rows"},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_candidates.html","id":null,"dir":"Reference","previous_headings":"","what":"Candidate pairs from LSH comparisons — lsh_candidates","title":"Candidate pairs from LSH comparisons — lsh_candidates","text":"Given data frame LSH buckets returned lsh, function returns potential candidates.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_candidates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Candidate pairs from LSH comparisons — lsh_candidates","text":"","code":"lsh_candidates(buckets)"},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_candidates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Candidate pairs from LSH comparisons — lsh_candidates","text":"buckets data frame returned lsh.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_candidates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Candidate pairs from LSH comparisons — lsh_candidates","text":"data frame candidate pairs.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_candidates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Candidate pairs from LSH comparisons — lsh_candidates","text":"","code":"dir <- system.file(\"extdata/legal\", package = \"textreuse\") minhash <- minhash_generator(200, seed = 234) corpus <- TextReuseCorpus(dir = dir,                           tokenizer = tokenize_ngrams, n = 5,                           minhash_func = minhash) buckets <- lsh(corpus, bands = 50) lsh_candidates(buckets) #> # A tibble: 1 × 3 #>   a            b            score #>   <chr>        <chr>        <dbl> #> 1 ca1851-match ny1850-match    NA"},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_compare.html","id":null,"dir":"Reference","previous_headings":"","what":"Compare candidates identified by LSH — lsh_compare","title":"Compare candidates identified by LSH — lsh_compare","text":"lsh_candidates identifies potential matches, estimate actual similarity documents. function takes data frame returned lsh_candidates applies comparison function documents corpus, thereby calculating document similarity score. Note since corpus minhash signatures rather hashes tokens , probably wish use tokenize calculate new hashes. can done just potentially similar documents. See package vignettes details.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_compare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compare candidates identified by LSH — lsh_compare","text":"","code":"lsh_compare(candidates, corpus, f, progress = interactive())"},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_compare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compare candidates identified by LSH — lsh_compare","text":"candidates data frame returned lsh_candidates. corpus TextReuseCorpus corpus used generate candidates. f comparison function jaccard_similarity. progress Display progress bar comparing documents.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_compare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compare candidates identified by LSH — lsh_compare","text":"data frame values calculated score.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_compare.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compare candidates identified by LSH — lsh_compare","text":"","code":"dir <- system.file(\"extdata/legal\", package = \"textreuse\") minhash <- minhash_generator(200, seed = 234) corpus <- TextReuseCorpus(dir = dir,                           tokenizer = tokenize_ngrams, n = 5,                           minhash_func = minhash) buckets <- lsh(corpus, bands = 50) candidates <- lsh_candidates(buckets) lsh_compare(candidates, corpus, jaccard_similarity) #> # A tibble: 1 × 3 #>   a            b            score #>   <chr>        <chr>        <dbl> #> 1 ca1851-match ny1850-match 0.450"},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_probability.html","id":null,"dir":"Reference","previous_headings":"","what":"Probability that a candidate pair will be detected with LSH — lsh_probability","title":"Probability that a candidate pair will be detected with LSH — lsh_probability","text":"Functions help choose correct parameters lsh minhash_generator functions. Use lsh_threshold determine minimum Jaccard similarity two documents likely considered match. Use lsh_probability determine probability pair documents known Jaccard similarity detected.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_probability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Probability that a candidate pair will be detected with LSH — lsh_probability","text":"","code":"lsh_probability(h, b, s)  lsh_threshold(h, b)"},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_probability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Probability that a candidate pair will be detected with LSH — lsh_probability","text":"h number minhash signatures. b number LSH bands. s Jaccard similarity.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_probability.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Probability that a candidate pair will be detected with LSH — lsh_probability","text":"Locality sensitive hashing returns list possible matches similar documents. likely pair documents detected possible match? h number minhash signatures, b number bands LSH function (implying number rows r = h / b), s actual Jaccard similarity two documents, probability p two documents marked candidate pair given equation. $$p = 1 - (1 - s^{r})^{b}$$ According MMDS, equation approximates S-curve. implies threshold (t) s approximated equation. $$t = \\frac{1}{b}^{\\frac{1}{r}}$$","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_probability.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Probability that a candidate pair will be detected with LSH — lsh_probability","text":"Jure Leskovec, Anand Rajaraman, Jeff Ullman,  Mining Massive Datasets  (Cambridge University Press, 2011), ch. 3.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_probability.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Probability that a candidate pair will be detected with LSH — lsh_probability","text":"","code":"# Threshold for default values lsh_threshold(h = 200, b = 40) #> [1] 0.4781762  # Probability for varying values of s lsh_probability(h = 200, b = 40, s = .25) #> [1] 0.03832775 lsh_probability(h = 200, b = 40, s = .50) #> [1] 0.7191538 lsh_probability(h = 200, b = 40, s = .75) #> [1] 0.9999803"},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Query a LSH cache for matches to a single document — lsh_query","title":"Query a LSH cache for matches to a single document — lsh_query","text":"function retrieves matches single document lsh_buckets object created lsh. See lsh_candidates retrieve pairs matches.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query a LSH cache for matches to a single document — lsh_query","text":"","code":"lsh_query(buckets, id)"},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query a LSH cache for matches to a single document — lsh_query","text":"buckets lsh_buckets object created lsh. id document ID find matches .","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query a LSH cache for matches to a single document — lsh_query","text":"lsh_candidates data frame matches document specified.","code":""},{"path":[]},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query a LSH cache for matches to a single document — lsh_query","text":"","code":"dir <- system.file(\"extdata/legal\", package = \"textreuse\") minhash <- minhash_generator(200, seed = 235) corpus <- TextReuseCorpus(dir = dir,                           tokenizer = tokenize_ngrams, n = 5,                           minhash_func = minhash) buckets <- lsh(corpus, bands = 50) lsh_query(buckets, \"ny1850-match\") #> # A tibble: 1 × 2 #>   a            b            #>   <chr>        <chr>        #> 1 ny1850-match ca1851-match"},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_subset.html","id":null,"dir":"Reference","previous_headings":"","what":"List of all candidates in a corpus — lsh_subset","title":"List of all candidates in a corpus — lsh_subset","text":"List candidates corpus","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_subset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List of all candidates in a corpus — lsh_subset","text":"","code":"lsh_subset(candidates)"},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_subset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List of all candidates in a corpus — lsh_subset","text":"candidates data frame candidate pairs lsh_candidates.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_subset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List of all candidates in a corpus — lsh_subset","text":"character vector document IDs candidate pairs,   used subset TextReuseCorpus.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/lsh_subset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List of all candidates in a corpus — lsh_subset","text":"","code":"dir <- system.file(\"extdata/legal\", package = \"textreuse\") minhash <- minhash_generator(200, seed = 234) corpus <- TextReuseCorpus(dir = dir,                           tokenizer = tokenize_ngrams, n = 5,                           minhash_func = minhash) buckets <- lsh(corpus, bands = 50) candidates <- lsh_candidates(buckets) lsh_subset(candidates) #> [1] \"ca1851-match\" \"ny1850-match\" corpus[lsh_subset(candidates)] #> TextReuseCorpus #> Number of documents: 2  #> hash_func : hash_string  #> minhash_func : minhash  #> tokenizer : tokenize_ngrams"},{"path":"https://docs.ropensci.org/textreuse/reference/minhash_generator.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a minhash function — minhash_generator","title":"Generate a minhash function — minhash_generator","text":"minhash value calculated hashing strings character vector integers selecting minimum value. Repeated minhash values generated using different hash functions: different hash functions created using performing bitwise XOR operation (bitwXor) vector random integers. Since vital random integers used document, function generates another function always use integers. returned function intended passed hash_func parameter TextReuseTextDocument.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/minhash_generator.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a minhash function — minhash_generator","text":"","code":"minhash_generator(n = 200, seed = NULL)"},{"path":"https://docs.ropensci.org/textreuse/reference/minhash_generator.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a minhash function — minhash_generator","text":"n number minhashes returned function generate. seed option parameter set seed used generating random numbers ensure minhash function used repeated applications.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/minhash_generator.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a minhash function — minhash_generator","text":"function take character vector return n minhashes.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/minhash_generator.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate a minhash function — minhash_generator","text":"Jure Leskovec, Anand Rajaraman, Jeff Ullman,   Mining Massive Datasets   (Cambridge University Press, 2011), ch. 3. See also Matthew Casperson,   \"Minhash    Dummies\" (November 14, 2013).","code":""},{"path":[]},{"path":"https://docs.ropensci.org/textreuse/reference/minhash_generator.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a minhash function — minhash_generator","text":"","code":"set.seed(253) minhash <- minhash_generator(10)  # Example with a TextReuseTextDocument file <- system.file(\"extdata/legal/ny1850-match.txt\", package = \"textreuse\") doc <- TextReuseTextDocument(file = file, hash_func = minhash,                              keep_tokens = TRUE) hashes(doc) #>  [1] -2132446047 -2134404886 -2138686164 -2143119093 -2140599954 -2145733916 #>  [7] -2136140472 -2140442115 -2145758614 -2145359786  # Example with a character vector is.character(tokens(doc)) #> [1] TRUE minhash(tokens(doc)) #>  [1] -2132446047 -2134404886 -2138686164 -2143119093 -2140599954 -2145733916 #>  [7] -2136140472 -2140442115 -2145758614 -2145359786"},{"path":"https://docs.ropensci.org/textreuse/reference/pairwise_candidates.html","id":null,"dir":"Reference","previous_headings":"","what":"Candidate pairs from pairwise comparisons — pairwise_candidates","title":"Candidate pairs from pairwise comparisons — pairwise_candidates","text":"Converts comparison matrix generated pairwise_compare data frame candidates matches.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/pairwise_candidates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Candidate pairs from pairwise comparisons — pairwise_candidates","text":"","code":"pairwise_candidates(m, directional = FALSE)"},{"path":"https://docs.ropensci.org/textreuse/reference/pairwise_candidates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Candidate pairs from pairwise comparisons — pairwise_candidates","text":"m matrix pairwise_compare. directional set value pairwise_compare.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/pairwise_candidates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Candidate pairs from pairwise comparisons — pairwise_candidates","text":"data frame containing non-NA values m.   Columns b IDs original corpus   passed comparison function. Column score score   returned comparison function.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/pairwise_candidates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Candidate pairs from pairwise comparisons — pairwise_candidates","text":"","code":"dir <- system.file(\"extdata/legal\", package = \"textreuse\") corpus <- TextReuseCorpus(dir = dir)  m1 <- pairwise_compare(corpus, ratio_of_matches, directional = TRUE) pairwise_candidates(m1, directional = TRUE) #> # A tibble: 6 × 3 #>   a              b                score #> * <chr>          <chr>            <dbl> #> 1 ca1851-match   ca1851-nomatch 0.0140  #> 2 ca1851-match   ny1850-match   0.695   #> 3 ca1851-nomatch ca1851-match   0.00550 #> 4 ca1851-nomatch ny1850-match   0.00508 #> 5 ny1850-match   ca1851-match   0.737   #> 6 ny1850-match   ca1851-nomatch 0.0140   m2 <- pairwise_compare(corpus, jaccard_similarity) pairwise_candidates(m2) #> # A tibble: 3 × 3 #>   a              b                score #> * <chr>          <chr>            <dbl> #> 1 ca1851-match   ca1851-nomatch 0.00353 #> 2 ca1851-match   ny1850-match   0.535   #> 3 ca1851-nomatch ny1850-match   0.00331"},{"path":"https://docs.ropensci.org/textreuse/reference/pairwise_compare.html","id":null,"dir":"Reference","previous_headings":"","what":"Pairwise comparisons among documents in a corpus — pairwise_compare","title":"Pairwise comparisons among documents in a corpus — pairwise_compare","text":"Given TextReuseCorpus containing documents class TextReuseTextDocument, function applies comparison function every pairing documents, returns matrix comparison scores.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/pairwise_compare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pairwise comparisons among documents in a corpus — pairwise_compare","text":"","code":"pairwise_compare(corpus, f, ..., directional = FALSE, progress = interactive())"},{"path":"https://docs.ropensci.org/textreuse/reference/pairwise_compare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pairwise comparisons among documents in a corpus — pairwise_compare","text":"corpus TextReuseCorpus. f function apply x y. ... Additional arguments passed f. directional comparison functions commutative, f(, b) == f(b, ) (e.g., jaccard_similarity). functions directional, f(, b) measures 's borrowing b, may f(b, ) (e.g., ratio_of_matches). directional FALSE, minimum number comparisons made, .e., upper triangle matrix. directional TRUE, directional comparisons measured. case, however, documents compared , .e., diagonal matrix. progress Display progress bar comparing documents.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/pairwise_compare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pairwise comparisons among documents in a corpus — pairwise_compare","text":"square matrix dimensions equal length corpus,   row column names set names documents corpus.   value NA matrix indicates comparison made.   cases directional comparisons, comparison reported  f(row, column).","code":""},{"path":[]},{"path":"https://docs.ropensci.org/textreuse/reference/pairwise_compare.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Pairwise comparisons among documents in a corpus — pairwise_compare","text":"","code":"dir <- system.file(\"extdata/legal\", package = \"textreuse\") corpus <- TextReuseCorpus(dir = dir) names(corpus) <- filenames(names(corpus))  # A non-directional comparison pairwise_compare(corpus, jaccard_similarity) #>                ca1851-match ca1851-nomatch ny1850-match #> ca1851-match             NA    0.003529412  0.534753363 #> ca1851-nomatch           NA             NA  0.003307607 #> ny1850-match             NA             NA           NA  # A directional comparison pairwise_compare(corpus, ratio_of_matches, directional = TRUE) #>                ca1851-match ca1851-nomatch ny1850-match #> ca1851-match             NA     0.01395349  0.695431472 #> ca1851-nomatch  0.005502063             NA  0.005076142 #> ny1850-match    0.737276479     0.01395349           NA"},{"path":"https://docs.ropensci.org/textreuse/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. NLP content, content<-, meta, meta<-","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/rehash.html","id":null,"dir":"Reference","previous_headings":"","what":"Recompute the hashes for a document or corpus — rehash","title":"Recompute the hashes for a document or corpus — rehash","text":"Given TextReuseTextDocument TextReuseCorpus, function recomputes either hashes minhashes function specified. implies retained tokens keep_tokens = TRUE parameter.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/rehash.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recompute the hashes for a document or corpus — rehash","text":"","code":"rehash(x, func, type = c(\"hashes\", \"minhashes\"))"},{"path":"https://docs.ropensci.org/textreuse/reference/rehash.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recompute the hashes for a document or corpus — rehash","text":"x TextReuseTextDocument TextReuseCorpus. func function either hash tokens generate minhash signature. See hash_string, minhash_generator. type Recompute hashes minhashes?","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/rehash.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recompute the hashes for a document or corpus — rehash","text":"modified TextReuseTextDocument  TextReuseCorpus.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/rehash.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Recompute the hashes for a document or corpus — rehash","text":"","code":"dir <- system.file(\"extdata/legal\", package = \"textreuse\") minhash1 <- minhash_generator(seed = 1) corpus <- TextReuseCorpus(dir = dir, minhash_func = minhash1, keep_tokens = TRUE) head(minhashes(corpus[[1]])) #> [1] -2123034802 -2135950804 -2141005830 -2143887934 -2140211929 -2141513215 minhash2 <- minhash_generator(seed = 2) corpus <- rehash(corpus, minhash2, type = \"minhashes\") head(minhashes(corpus[[2]])) #> [1] -2139907006 -2113028606 -2143609422 -2087210065 -2050121581 -2049620020"},{"path":"https://docs.ropensci.org/textreuse/reference/similarity-functions.html","id":null,"dir":"Reference","previous_headings":"","what":"Measure similarity/dissimilarity in documents — similarity-functions","title":"Measure similarity/dissimilarity in documents — similarity-functions","text":"set functions take two sets bag words measure similarity dissimilarity.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/similarity-functions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Measure similarity/dissimilarity in documents — similarity-functions","text":"","code":"jaccard_similarity(a, b)  jaccard_dissimilarity(a, b)  jaccard_bag_similarity(a, b)  ratio_of_matches(a, b)"},{"path":"https://docs.ropensci.org/textreuse/reference/similarity-functions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Measure similarity/dissimilarity in documents — similarity-functions","text":"first set (bag) compared. origin bag directional comparisons. b second set (bag) compared. destination bag directional comparisons.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/similarity-functions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Measure similarity/dissimilarity in documents — similarity-functions","text":"functions jaccard_similarity   jaccard_dissimilarity provide Jaccard measures similarity   dissimilarity two sets. coefficients numbers   0 1. similarity coefficient, higher   number similar two sets . applied two documents   class TextReuseTextDocument, hashes documents   compared. function can passed objects class accepted   set functions base R. possible, instance, pass   function two character vectors comprised word, line, sentence,   paragraph tokens, character vectors hashed integers. Jaccard similarity coeffecient defined follows: $$J(, B) = \\frac{ | \\cap B | }{ | \\cup B | }$$ Jaccard dissimilarity simply $$1 - J(, B)$$ function jaccard_bag_similarity treats b   bags rather sets, result fraction numerator   sum matching element counted minimum number times   appears bag, denominator sum lengths   bags. maximum value Jaccard bag similarity 0.5. function ratio_of_matches finds ratio number   items b also total number items   b. Note similarity measure directional: measures   much b borrows , says nothing much   borrows b.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/similarity-functions.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Measure similarity/dissimilarity in documents — similarity-functions","text":"Jure Leskovec, Anand Rajaraman, Jeff Ullman,   Mining Massive Datasets   (Cambridge University Press, 2011).","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/similarity-functions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Measure similarity/dissimilarity in documents — similarity-functions","text":"","code":"jaccard_similarity(1:6, 3:10) #> [1] 0.4 jaccard_dissimilarity(1:6, 3:10) #> [1] 0.6  a <- c(\"a\", \"a\", \"a\", \"b\") b <- c(\"a\", \"a\", \"b\", \"b\", \"c\") jaccard_similarity(a, b) #> [1] 0.6666667 jaccard_bag_similarity(a, b) #> [1] 0.3333333 ratio_of_matches(a, b) #> [1] 0.8 ratio_of_matches(b, a) #> [1] 1  ny         <- system.file(\"extdata/legal/ny1850-match.txt\", package = \"textreuse\") ca_match   <- system.file(\"extdata/legal/ca1851-match.txt\", package = \"textreuse\") ca_nomatch <- system.file(\"extdata/legal/ca1851-nomatch.txt\", package = \"textreuse\")  ny         <- TextReuseTextDocument(file = ny,                                     meta = list(id = \"ny\")) ca_match   <- TextReuseTextDocument(file = ca_match,                                     meta = list(id = \"ca_match\")) ca_nomatch <- TextReuseTextDocument(file = ca_nomatch,                                     meta = list(id = \"ca_nomatch\"))  # These two should have higher similarity scores jaccard_similarity(ny, ca_match) #> [1] 0.5347534 ratio_of_matches(ny, ca_match) #> [1] 0.7372765  # These two should have lower similarity scores jaccard_similarity(ny, ca_nomatch) #> [1] 0.003307607 ratio_of_matches(ny, ca_nomatch) #> [1] 0.01395349"},{"path":"https://docs.ropensci.org/textreuse/reference/textreuse-package.html","id":null,"dir":"Reference","previous_headings":"","what":"textreuse: Detect Text Reuse and Document Similarity — textreuse-package","title":"textreuse: Detect Text Reuse and Document Similarity — textreuse-package","text":"Tools measuring similarity among documents detecting     passages reused. Implements shingled n-gram, skip n-gram,     tokenizers; similarity/dissimilarity functions; pairwise     comparisons; minhash locality sensitive hashing algorithms;     version Smith-Waterman local alignment algorithm suitable     natural language.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/textreuse-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"textreuse: Detect Text Reuse and Document Similarity — textreuse-package","text":"best place begin package introductory vignette. vignette(\"textreuse-introduction\", package = \"textreuse\") reading vignette, \"pairwise\" \"minhash\" vignettes introduce specific paths working package. vignette(\"textreuse-pairwise\", package = \"textreuse\") vignette(\"textreuse-minhash\", package = \"textreuse\") vignette(\"textreuse-alignment\", package = \"textreuse\") Another good place begin package documentation loading documents (TextReuseTextDocument TextReuseCorpus), tokenizers, similarity functions, locality-sensitive hashing.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/textreuse-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"textreuse: Detect Text Reuse and Document Similarity — textreuse-package","text":"sample data provided extdata/legal directory   taken   corpus    American Tract Society publications nineteen-century,   gathered Internet Archive. sample data provided extdata/legal directory, taken   following nineteenth-century codes civil procedure   California New York. Final Report Commissioners Practice Pleadings, 2   Documents Assembly New York, 73rd Sess., . 16, (1850):   243-250, sections 597-613.   Google    Books. Act Regulate Proceedings Civil Cases, 1851 California   Laws 51, 51-53 sections 4-17; 101, sections 313-316.   Google    Books.","code":""},{"path":[]},{"path":"https://docs.ropensci.org/textreuse/reference/textreuse-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"textreuse: Detect Text Reuse and Document Similarity — textreuse-package","text":"Maintainer: Lincoln Mullen lincoln@lincolnmullen.com (ORCID)","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/tokenize.html","id":null,"dir":"Reference","previous_headings":"","what":"Recompute the tokens for a document or corpus — tokenize","title":"Recompute the tokens for a document or corpus — tokenize","text":"Given TextReuseTextDocument TextReuseCorpus, function recomputes tokens hashes functions specified. Optionally, can also recompute minhash signatures.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/tokenize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recompute the tokens for a document or corpus — tokenize","text":"","code":"tokenize(   x,   tokenizer,   ...,   hash_func = hash_string,   minhash_func = NULL,   keep_tokens = FALSE,   keep_text = TRUE )"},{"path":"https://docs.ropensci.org/textreuse/reference/tokenize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recompute the tokens for a document or corpus — tokenize","text":"x TextReuseTextDocument TextReuseCorpus. tokenizer function split text tokens. See tokenizers. ... Arguments passed tokenizer. hash_func function hash tokens. See hash_string. minhash_func function create minhash signatures. See minhash_generator. keep_tokens tokens saved document returned discarded? keep_text text saved document returned discarded?","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/tokenize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Recompute the tokens for a document or corpus — tokenize","text":"modified TextReuseTextDocument  TextReuseCorpus.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/tokenize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Recompute the tokens for a document or corpus — tokenize","text":"","code":"dir <- system.file(\"extdata/legal\", package = \"textreuse\") corpus <- TextReuseCorpus(dir = dir, tokenizer = NULL) corpus <- tokenize(corpus, tokenize_ngrams) head(tokens(corpus[[1]])) #> [1] \"4 every action\"      \"every action shall\"  \"action shall be\"     #> [4] \"shall be prosecuted\" \"be prosecuted in\"    \"prosecuted in the\""},{"path":"https://docs.ropensci.org/textreuse/reference/tokenizers.html","id":null,"dir":"Reference","previous_headings":"","what":"Split texts into tokens — tokenizers","title":"Split texts into tokens — tokenizers","text":"functions turn text tokens. tokenize_ngrams functions returns shingled n-grams.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/tokenizers.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split texts into tokens — tokenizers","text":"","code":"tokenize_words(string, lowercase = TRUE)  tokenize_sentences(string, lowercase = TRUE)  tokenize_ngrams(string, lowercase = TRUE, n = 3)  tokenize_skip_ngrams(string, lowercase = TRUE, n = 3, k = 1)"},{"path":"https://docs.ropensci.org/textreuse/reference/tokenizers.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split texts into tokens — tokenizers","text":"string character vector length 1 tokenized. lowercase tokens made lower case? n n-gram tokenizers, number words n-gram. k skip n-gram tokenizer, maximum skip distance words. function compute skip n-grams 0 k.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/tokenizers.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Split texts into tokens — tokenizers","text":"character vector containing tokens.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/tokenizers.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Split texts into tokens — tokenizers","text":"functions strip punctuation.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/tokenizers.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Split texts into tokens — tokenizers","text":"","code":"dylan <- \"How many roads must a man walk down? The answer is blowin' in the wind.\" tokenize_words(dylan) #>  [1] \"how\"    \"many\"   \"roads\"  \"must\"   \"a\"      \"man\"    \"walk\"   \"down\"   #>  [9] \"the\"    \"answer\" \"is\"     \"blowin\" \"in\"     \"the\"    \"wind\"   tokenize_sentences(dylan) #> [1] \"how many roads must a man walk down\" \"the answer is blowin in the wind\"    tokenize_ngrams(dylan, n = 2) #>  [1] \"how many\"   \"many roads\" \"roads must\" \"must a\"     \"a man\"      #>  [6] \"man walk\"   \"walk down\"  \"down the\"   \"the answer\" \"answer is\"  #> [11] \"is blowin\"  \"blowin in\"  \"in the\"     \"the wind\"   tokenize_skip_ngrams(dylan, n = 3, k = 2) #>  [1] \"how must walk\"      \"many a down\"        \"roads man the\"      #>  [4] \"must walk answer\"   \"a down is\"          \"man the blowin\"     #>  [7] \"walk answer in\"     \"down is the\"        \"the blowin wind\"    #> [10] \"how roads a\"        \"many must man\"      \"roads a walk\"       #> [13] \"must man down\"      \"a walk the\"         \"man down answer\"    #> [16] \"walk the is\"        \"down answer blowin\" \"the is in\"          #> [19] \"answer blowin the\"  \"is in wind\"         \"how many roads\"     #> [22] \"many roads must\"    \"roads must a\"       \"must a man\"         #> [25] \"a man walk\"         \"man walk down\"      \"walk down the\"      #> [28] \"down the answer\"    \"the answer is\"      \"answer is blowin\"   #> [31] \"is blowin in\"       \"blowin in the\"      \"in the wind\""},{"path":"https://docs.ropensci.org/textreuse/reference/wordcount.html","id":null,"dir":"Reference","previous_headings":"","what":"Count words — wordcount","title":"Count words — wordcount","text":"function counts words text, example, character vector, TextReuseTextDocument, object inherits TextDocument, documents TextReuseCorpus.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/wordcount.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count words — wordcount","text":"","code":"wordcount(x)"},{"path":"https://docs.ropensci.org/textreuse/reference/wordcount.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count words — wordcount","text":"x object containing text.","code":""},{"path":"https://docs.ropensci.org/textreuse/reference/wordcount.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count words — wordcount","text":"integer vector word count.","code":""},{"path":"https://docs.ropensci.org/textreuse/news/index.html","id":"textreuse-015","dir":"Changelog","previous_headings":"","what":"textreuse 0.1.5","title":"textreuse 0.1.5","text":"CRAN release: 2020-05-15 Updates due dplyr 1.0.0 release","code":""},{"path":"https://docs.ropensci.org/textreuse/news/index.html","id":"textreuse-014","dir":"Changelog","previous_headings":"","what":"textreuse 0.1.4","title":"textreuse 0.1.4","text":"CRAN release: 2016-11-28 Preventative maintenance release avoid failing tests new version BH released.","code":""},{"path":"https://docs.ropensci.org/textreuse/news/index.html","id":"textreuse-013","dir":"Changelog","previous_headings":"","what":"textreuse 0.1.3","title":"textreuse 0.1.3","text":"CRAN release: 2016-03-28 Preventative maintenance release avoid failing tests new versions dplyr testthat packages released.","code":""},{"path":"https://docs.ropensci.org/textreuse/news/index.html","id":"textreuse-012","dir":"Changelog","previous_headings":"","what":"textreuse 0.1.2","title":"textreuse 0.1.2","text":"CRAN release: 2015-11-06 Fix memory error shingle_ngrams() Fix tests retokenizing Windows informative error message using lsh() corpora without minhashes","code":""},{"path":"https://docs.ropensci.org/textreuse/news/index.html","id":"textreuse-011","dir":"Changelog","previous_headings":"","what":"textreuse 0.1.1","title":"textreuse 0.1.1","text":"CRAN release: 2015-11-04 Fix progress bars vignettes","code":""},{"path":"https://docs.ropensci.org/textreuse/news/index.html","id":"textreuse-010","dir":"Changelog","previous_headings":"","what":"textreuse 0.1.0","title":"textreuse 0.1.0","text":"CRAN release: 2015-10-31 Initial release","code":""}]
